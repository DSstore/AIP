{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "functions",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental import preprocessing"
      ],
      "metadata": {
        "id": "L-cYyDV9c9pZ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AFu4h_EX_PHS"
      },
      "outputs": [],
      "source": [
        "# View an image as a function\n",
        "\n",
        "def view_random_image(target_dir, target_class):\n",
        "  # Setup target directory (we'll view images from here)\n",
        "  target_folder = target_dir+target_class\n",
        "\n",
        "  # Get a random image path\n",
        "  random_image = random.sample(os.listdir(target_folder), 1)\n",
        "  print(random_image)\n",
        "\n",
        "  # Read in the image and plot it using matplotlib\n",
        "  img = mpimg.imread(target_folder + \"/\" + random_image[0])\n",
        "  plt.imshow(img)\n",
        "  plt.title(target_class)\n",
        "  plt.axis(\"off\");\n",
        "\n",
        "  print(f\"Image shape: {img.shape}\") # show the shape of the image\n",
        "\n",
        "  return img    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the validation and training data separately\n",
        "def plot_loss_curves(history):\n",
        "  \"\"\"\n",
        "  Returns separate loss curves for training and validation metrics.\n",
        "  \"\"\" \n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  accuracy = history.history['accuracy']\n",
        "  val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "  epochs = range(len(history.history['loss']))\n",
        "\n",
        "  # Plot loss\n",
        "  plt.plot(epochs, loss, label='training_loss')\n",
        "  plt.plot(epochs, val_loss, label='val_loss')\n",
        "  plt.title('Loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.legend()\n",
        "\n",
        "  # Plot accuracy\n",
        "  plt.figure()\n",
        "  plt.plot(epochs, accuracy, label='training_accuracy')\n",
        "  plt.plot(epochs, val_accuracy, label='val_accuracy')\n",
        "  plt.title('Accuracy')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.legend();"
      ],
      "metadata": {
        "id": "9hN9tn4h_bRg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to import an image and resize it to be able to be used with our model\n",
        "def load_and_prep_image(filename, img_shape=224):\n",
        "  \"\"\"\n",
        "  Reads an image from filename, turns it into a tensor\n",
        "  and reshapes it to (img_shape, img_shape, colour_channel).\n",
        "  \"\"\"\n",
        "  # Read in target file (an image)\n",
        "  img = tf.io.read_file(filename)\n",
        "\n",
        "  # Decode the read file into a tensor & ensure 3 colour channels \n",
        "  # (our model is trained on images with 3 colour channels and sometimes images have 4 colour channels)\n",
        "  img = tf.image.decode_image(img, channels=3)\n",
        "\n",
        "  # Resize the image (to the same size our model was trained on)\n",
        "  img = tf.image.resize(img, size = [img_shape, img_shape])\n",
        "\n",
        "  # Rescale the image (get all values between 0 and 1)\n",
        "  img = img/255.\n",
        "  return img"
      ],
      "metadata": {
        "id": "Eo7JK01Ic1K1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pred_and_plot(model, filename, class_names):\n",
        "  \"\"\"\n",
        "  Imports an image located at filename, makes a prediction on it with\n",
        "  a trained model and plots the image with the predicted class as the title.\n",
        "  \"\"\"\n",
        "  # Import the target image and preprocess it\n",
        "  img = load_and_prep_image(filename)\n",
        "\n",
        "  # Make a prediction\n",
        "  pred = model.predict(tf.expand_dims(img, axis=0))\n",
        "\n",
        "  # Get the predicted class\n",
        "  pred_class = class_names[int(tf.round(pred))]\n",
        "\n",
        "  # Plot the image and predicted class\n",
        "  plt.imshow(img)\n",
        "  plt.title(f\"Prediction: {pred_class}\")\n",
        "  plt.axis(False);"
      ],
      "metadata": {
        "id": "5l-OliOtcNzL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_processing(filepath):\n",
        "    \"\"\" Create a DataFrame with the filepath and the labels of the pictures\n",
        "    \"\"\"\n",
        "\n",
        "    labels = [str(filepath[i]).split(\"/\")[-2] \\\n",
        "              for i in range(len(filepath))]\n",
        "\n",
        "    filepath = pd.Series(filepath, name='Filepath').astype(str)\n",
        "    labels = pd.Series(labels, name='Label')\n",
        "\n",
        "    # Concatenate filepaths and labels\n",
        "    df = pd.concat([filepath, labels], axis=1)\n",
        "\n",
        "    # Shuffle the DataFrame and reset index\n",
        "    df = df.sample(frac=1).reset_index(drop = True)\n",
        "    \n",
        "    return df"
      ],
      "metadata": {
        "id": "eimtRVE_cjbv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip the downloaded file\n",
        "def unzip_data(filename):\n",
        "  \"\"\"\n",
        "  Utility function to unzip a zipped file.\n",
        "  \"\"\"\n",
        "  zip_ref = zipfile.ZipFile(filename, \"r\")\n",
        "  zip_ref.extractall()\n",
        "  zip_ref.close()"
      ],
      "metadata": {
        "id": "mKNqWmbrNxIv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup data inputs\n",
        "IMG_SIZE = (224, 224)\n",
        "\n",
        "def create_data_loaders(train_dir, test_dir, image_size=IMG_SIZE):\n",
        "  \"\"\"\n",
        "  Creates a training and test image BatchDataset from train_dir and test_dir.\n",
        "  \"\"\"\n",
        "  train_data = tf.keras.preprocessing.image_dataset_from_directory(train_dir,\n",
        "                                                                  label_mode=\"categorical\",\n",
        "                                                                  image_size=image_size)\n",
        "  # Note: the test data is the same as the previous experiment, we could\n",
        "  # skip creating this, but we'll leave this here to practice.\n",
        "  test_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir,\n",
        "                                                                  label_mode=\"categorical\",\n",
        "                                                                  image_size=image_size)\n",
        "  \n",
        "  return train_data, test_data"
      ],
      "metadata": {
        "id": "3COpfbj_N-rf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a data augmentation stage with horizontal flipping, rotations, zooms\n",
        "data_augmentation = keras.Sequential([\n",
        "  preprocessing.RandomFlip(\"horizontal\"),\n",
        "  preprocessing.RandomRotation(0.2),\n",
        "  preprocessing.RandomZoom(0.2),\n",
        "  preprocessing.RandomHeight(0.2),\n",
        "  preprocessing.RandomWidth(0.2),\n",
        "  # preprocessing.Rescaling(1./255) # keep for ResNet50V2, remove for EfficientNetB0\n",
        "], name =\"data_augmentation\")\n",
        "\n",
        "# Setup input shape and base model, freezing the base model layers\n",
        "INPUT_SHAPE = (224, 224, 3)\n",
        "BASE_MODEL = tf.keras.applications.EfficientNetB0(include_top=False)\n",
        "\n",
        "def create_model(input_shape=INPUT_SHAPE, base_model=BASE_MODEL, num_classes=10):\n",
        "  # Fine-tune?\n",
        "  base_model.trainable = False\n",
        "\n",
        "  # Create input layer\n",
        "  inputs = layers.Input(shape=input_shape, name=\"input_layer\")\n",
        "\n",
        "  # Add in data augmentation Sequential model as a layer\n",
        "  x = data_augmentation(inputs)\n",
        "\n",
        "  # Give base_model inputs (after augmentation) and don't train it\n",
        "  x = base_model(x, training=False)\n",
        "\n",
        "  # Pool output features of base model\n",
        "  x = layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\n",
        "\n",
        "  # Put a dense layer on as the output\n",
        "  outputs = layers.Dense(num_classes, activation=\"softmax\", name=\"output_layer\")(x)\n",
        "\n",
        "  # Make a model with inputs and outputs\n",
        "  model = keras.Model(inputs, outputs)\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(loss=\"categorical_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "  \n",
        "  return model"
      ],
      "metadata": {
        "id": "skbu8C7zOE90"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}